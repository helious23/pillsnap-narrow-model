# ğŸš€ Phase 5: ëª¨ë¸ ë°°í¬ ë° í…ŒìŠ¤íŠ¸ ìƒì„¸ ê³„íš

## ğŸ¯ ëª©í‘œ
í•™ìŠµëœ ëª¨ë¸ì„ **í”„ë¡œë•ì…˜ í™˜ê²½ì— ë°°í¬**í•˜ê³  **ì‹¤ì œ ì•½êµ­ í™˜ê²½ì—ì„œ ê²€ì¦** ì™„ë£Œ

## ğŸ“… ì¼ì •: 2024-11-09 ~ 2024-11-12 (4ì¼)

## ğŸ—ï¸ ë°°í¬ ì•„í‚¤í…ì²˜

```mermaid
graph LR
    subgraph "ëª¨ë¸ ë³€í™˜"
        A[PyTorch Model] --> B[ONNX Export]
        B --> C[Optimization]
        C --> D[Quantization]
    end

    subgraph "ì¶”ë¡  ì„œë²„"
        D --> E[ONNX Runtime]
        E --> F[FastAPI Server]
        F --> G[100-class Endpoint]
    end

    subgraph "í´ë¼ì´ì–¸íŠ¸"
        G --> H[BFF Server]
        H --> I[Flutter App]
        I --> J[ì•½êµ­ í…ŒìŠ¤íŠ¸]
    end
```

## ğŸ“‹ ìƒì„¸ ì‘ì—…

### Part A: ëª¨ë¸ ë°°í¬

---

#### Task 1: PyTorch to ONNX ë³€í™˜

**ONNX Export ìŠ¤í¬ë¦½íŠ¸**:
```python
# scripts/export_to_onnx.py

import torch
import torch.onnx
import onnx
import onnxruntime as ort
from onnxruntime.quantization import quantize_dynamic, QuantType
import numpy as np
import time

class ModelExporter:
    def __init__(self, checkpoint_path, output_dir):
        self.checkpoint_path = checkpoint_path
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # ëª¨ë¸ ë¡œë“œ
        self.model = self.load_model()
        self.model.eval()

    def load_model(self):
        """PyTorch ëª¨ë¸ ë¡œë“œ"""
        checkpoint = torch.load(self.checkpoint_path)

        # ëª¨ë¸ ìƒì„±
        model = timm.create_model(
            'tf_efficientnetv2_s',
            num_classes=100,
            pretrained=False
        )

        # ê°€ì¤‘ì¹˜ ë¡œë“œ
        model.load_state_dict(checkpoint['model_state_dict'])

        print(f"âœ… Model loaded from epoch {checkpoint['epoch']}")
        print(f"   Validation accuracy: {checkpoint['accuracy']*100:.2f}%")

        return model

    def export_to_onnx(self):
        """ONNX í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""

        # ë”ë¯¸ ì…ë ¥ ìƒì„±
        dummy_input = torch.randn(1, 3, 512, 512)

        # Export ê²½ë¡œ
        onnx_path = self.output_dir / 'pillsnap_narrow_model.onnx'

        # ONNX Export
        torch.onnx.export(
            self.model,
            dummy_input,
            onnx_path,
            export_params=True,
            opset_version=12,
            do_constant_folding=True,
            input_names=['input'],
            output_names=['output'],
            dynamic_axes={
                'input': {0: 'batch_size'},
                'output': {0: 'batch_size'}
            },
            verbose=False
        )

        print(f"âœ… ONNX model exported to: {onnx_path}")
        return onnx_path

    def optimize_onnx(self, onnx_path):
        """ONNX ëª¨ë¸ ìµœì í™”"""

        # ONNX ëª¨ë¸ ë¡œë“œ
        onnx_model = onnx.load(onnx_path)

        # ìµœì í™”ëœ ê²½ë¡œ
        optimized_path = self.output_dir / 'pillsnap_narrow_model_optimized.onnx'

        # Graph optimization
        from onnxruntime.transformers import optimizer
        optimized_model = optimizer.optimize_model(
            str(onnx_path),
            model_type='bert',  # Vision transformerì™€ ìœ ì‚¬
            num_heads=0,
            hidden_size=0,
            opt_level=2
        )

        # ì €ì¥
        onnx.save(optimized_model.model, optimized_path)

        # íŒŒì¼ í¬ê¸° ë¹„êµ
        original_size = onnx_path.stat().st_size / (1024*1024)  # MB
        optimized_size = optimized_path.stat().st_size / (1024*1024)  # MB

        print(f"âœ… Model optimized:")
        print(f"   Original: {original_size:.2f} MB")
        print(f"   Optimized: {optimized_size:.2f} MB")
        print(f"   Reduction: {(1 - optimized_size/original_size)*100:.1f}%")

        return optimized_path

    def quantize_model(self, onnx_path):
        """ë™ì  ì–‘ìí™” ì ìš©"""

        quantized_path = self.output_dir / 'pillsnap_narrow_model_quantized.onnx'

        # ë™ì  ì–‘ìí™”
        quantize_dynamic(
            str(onnx_path),
            str(quantized_path),
            weight_type=QuantType.QUInt8
        )

        # íŒŒì¼ í¬ê¸° ë¹„êµ
        original_size = onnx_path.stat().st_size / (1024*1024)
        quantized_size = quantized_path.stat().st_size / (1024*1024)

        print(f"âœ… Model quantized:")
        print(f"   Original: {original_size:.2f} MB")
        print(f"   Quantized: {quantized_size:.2f} MB")
        print(f"   Reduction: {(1 - quantized_size/original_size)*100:.1f}%")

        return quantized_path

    def validate_onnx(self, onnx_path):
        """ONNX ëª¨ë¸ ê²€ì¦"""

        # ONNX ëª¨ë¸ ê²€ì¦
        onnx_model = onnx.load(onnx_path)
        onnx.checker.check_model(onnx_model)

        # ONNX Runtimeìœ¼ë¡œ í…ŒìŠ¤íŠ¸
        session = ort.InferenceSession(str(onnx_path))

        # í…ŒìŠ¤íŠ¸ ì…ë ¥
        test_input = np.random.randn(1, 3, 512, 512).astype(np.float32)

        # ì¶”ë¡  ì‹¤í–‰
        outputs = session.run(None, {'input': test_input})

        # ê²°ê³¼ í™•ì¸
        assert outputs[0].shape == (1, 100), "Output shape mismatch"
        assert np.isfinite(outputs[0]).all(), "NaN or Inf in outputs"

        print(f"âœ… ONNX model validation passed")
        print(f"   Output shape: {outputs[0].shape}")
        print(f"   Output range: [{outputs[0].min():.4f}, {outputs[0].max():.4f}]")

        return True

    def benchmark_performance(self, onnx_path):
        """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""

        # ONNX Runtime ì„¸ì…˜ ìƒì„±
        providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']
        session = ort.InferenceSession(str(onnx_path), providers=providers)

        # í…ŒìŠ¤íŠ¸ ì…ë ¥
        test_input = np.random.randn(1, 3, 512, 512).astype(np.float32)

        # Warmup
        for _ in range(10):
            session.run(None, {'input': test_input})

        # ë²¤ì¹˜ë§ˆí¬
        times = []
        for _ in range(100):
            start = time.perf_counter()
            outputs = session.run(None, {'input': test_input})
            end = time.perf_counter()
            times.append((end - start) * 1000)  # ms

        # í†µê³„
        times = np.array(times)
        print(f"âœ… Performance benchmark:")
        print(f"   Mean: {times.mean():.2f} ms")
        print(f"   Std: {times.std():.2f} ms")
        print(f"   P50: {np.percentile(times, 50):.2f} ms")
        print(f"   P95: {np.percentile(times, 95):.2f} ms")
        print(f"   P99: {np.percentile(times, 99):.2f} ms")

        return times

    def export_all(self):
        """ì „ì²´ export íŒŒì´í”„ë¼ì¸"""

        print("="*50)
        print("Starting model export pipeline...")
        print("="*50)

        # 1. ONNX Export
        onnx_path = self.export_to_onnx()
        self.validate_onnx(onnx_path)

        # 2. ìµœì í™”
        optimized_path = self.optimize_onnx(onnx_path)
        self.validate_onnx(optimized_path)

        # 3. ì–‘ìí™”
        quantized_path = self.quantize_model(optimized_path)
        self.validate_onnx(quantized_path)

        # 4. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
        print("\nğŸ“Š Benchmark Results:")
        print("-"*50)

        print("\n[Original ONNX]")
        self.benchmark_performance(onnx_path)

        print("\n[Optimized ONNX]")
        self.benchmark_performance(optimized_path)

        print("\n[Quantized ONNX]")
        self.benchmark_performance(quantized_path)

        # 5. ë©”íƒ€ë°ì´í„° ìƒì„±
        self.create_metadata()

        print("\nâœ… Export pipeline completed successfully!")

    def create_metadata(self):
        """ëª¨ë¸ ë©”íƒ€ë°ì´í„° ìƒì„±"""

        metadata = {
            'model_version': '1.0.0',
            'created_date': datetime.now().isoformat(),
            'architecture': 'EfficientNetV2-S',
            'num_classes': 100,
            'input_size': 512,
            'preprocessing': UnifiedPreprocessor().get_config(),
            'training_accuracy': {
                'top1': 0.85,  # From training
                'top5': 0.95
            },
            'class_mapping': 'class_mapping.json',
            'files': {
                'original': 'pillsnap_narrow_model.onnx',
                'optimized': 'pillsnap_narrow_model_optimized.onnx',
                'quantized': 'pillsnap_narrow_model_quantized.onnx'
            }
        }

        metadata_path = self.output_dir / 'model_metadata.json'
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)

        print(f"âœ… Metadata saved to: {metadata_path}")

# ì‹¤í–‰
if __name__ == '__main__':
    exporter = ModelExporter(
        checkpoint_path='checkpoints/best_model.pth',
        output_dir='artifacts/export'
    )
    exporter.export_all()
```

---

#### Task 2: ì¶”ë¡ ì„œë²„ í†µí•©

**100ê°œ í´ë˜ìŠ¤ ì „ìš© ì—”ë“œí¬ì¸íŠ¸**:
```python
# pillsnap_inference/src/endpoints/narrow_model.py

from fastapi import APIRouter, File, UploadFile, HTTPException
from typing import List, Dict
import onnxruntime as ort
import numpy as np
import json

router = APIRouter(prefix="/v1/narrow", tags=["narrow_model"])

class NarrowModelInference:
    def __init__(self):
        # ëª¨ë¸ ë¡œë“œ
        self.model_path = 'models/pillsnap_narrow_model_quantized.onnx'
        self.session = self._load_model()

        # í´ë˜ìŠ¤ ë§¤í•‘ ë¡œë“œ
        with open('models/class_mapping.json', 'r') as f:
            self.class_mapping = json.load(f)

        # ì „ì²˜ë¦¬ê¸°
        self.preprocessor = UnifiedPreprocessor()

    def _load_model(self):
        """ONNX ëª¨ë¸ ë¡œë“œ"""
        providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']

        session = ort.InferenceSession(
            self.model_path,
            providers=providers
        )

        print(f"âœ… Narrow model loaded: {self.model_path}")
        return session

    def predict(self, image_bytes: bytes) -> Dict:
        """ë‹¨ì¼ ì´ë¯¸ì§€ ì¶”ë¡ """

        # ì „ì²˜ë¦¬
        nparr = np.frombuffer(image_bytes, np.uint8)
        image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        tensor = self.preprocessor.process(image)
        batch = tensor.unsqueeze(0).numpy()

        # ì¶”ë¡ 
        outputs = self.session.run(None, {'input': batch})[0]
        logits = outputs[0]

        # Softmax
        probs = np.exp(logits) / np.sum(np.exp(logits))

        # Top-5 ê²°ê³¼
        top5_idx = np.argsort(probs)[-5:][::-1]
        top5_probs = probs[top5_idx]

        # ê²°ê³¼ ìƒì„±
        results = []
        for idx, prob in zip(top5_idx, top5_probs):
            kcode = self.class_mapping['idx_to_class'][str(idx)]
            metadata = self.class_mapping['metadata'][kcode]

            results.append({
                'rank': len(results) + 1,
                'kcode': kcode,
                'drug_name': metadata['drug_name'],
                'confidence': float(prob),
                'manufacturer': metadata.get('manufacturer', '')
            })

        return {
            'success': True,
            'model_version': '1.0.0',
            'num_classes': 100,
            'results': results,
            'top1': results[0] if results else None,
            'processing_time_ms': 0  # Will be filled by middleware
        }

# ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
narrow_model = NarrowModelInference()

@router.post("/predict")
async def predict_narrow(
    file: UploadFile = File(...)
) -> Dict:
    """
    100ê°œ ì•½í’ˆ ì „ìš© ëª¨ë¸ë¡œ ì˜ˆì¸¡

    Args:
        file: ì•½í’ˆ ì´ë¯¸ì§€

    Returns:
        Top-5 ì˜ˆì¸¡ ê²°ê³¼
    """
    try:
        # ì´ë¯¸ì§€ ì½ê¸°
        image_bytes = await file.read()

        # í¬ê¸° ê²€ì¦
        if len(image_bytes) > 10 * 1024 * 1024:  # 10MB
            raise HTTPException(400, "File size exceeds 10MB")

        # ì˜ˆì¸¡
        result = narrow_model.predict(image_bytes)

        return result

    except Exception as e:
        raise HTTPException(500, f"Prediction failed: {str(e)}")

@router.post("/batch_predict")
async def batch_predict_narrow(
    files: List[UploadFile] = File(...)
) -> Dict:
    """
    ë°°ì¹˜ ì˜ˆì¸¡ (ìµœëŒ€ 10ê°œ)
    """
    if len(files) > 10:
        raise HTTPException(400, "Maximum 10 images per batch")

    results = []
    for file in files:
        image_bytes = await file.read()
        result = narrow_model.predict(image_bytes)
        results.append(result)

    return {
        'success': True,
        'batch_size': len(files),
        'results': results
    }

@router.get("/info")
async def model_info() -> Dict:
    """ëª¨ë¸ ì •ë³´ ì¡°íšŒ"""
    return {
        'model_type': 'narrow',
        'architecture': 'EfficientNetV2-S',
        'num_classes': 100,
        'input_size': 512,
        'model_version': '1.0.0',
        'file_size_mb': 35.2,  # Quantized size
        'supported_formats': ['JPEG', 'PNG'],
        'max_file_size_mb': 10,
        'coverage': 'Top 100 Korean pharmacy drugs'
    }

@router.get("/classes")
async def get_classes() -> Dict:
    """ì§€ì›í•˜ëŠ” 100ê°œ ì•½í’ˆ ë¦¬ìŠ¤íŠ¸"""
    drugs = []

    for kcode, metadata in narrow_model.class_mapping['metadata'].items():
        drugs.append({
            'kcode': kcode,
            'drug_name': metadata['drug_name'],
            'manufacturer': metadata.get('manufacturer', ''),
            'form': metadata.get('form', '')
        })

    return {
        'total': len(drugs),
        'drugs': drugs
    }
```

**ë©”ì¸ ì•± í†µí•©**:
```python
# pillsnap_inference/src/main.py ì— ì¶”ê°€

from endpoints.narrow_model import router as narrow_router

app.include_router(narrow_router)

# Startup event
@app.on_event("startup")
async def startup_event():
    # Narrow ëª¨ë¸ ì‚¬ì „ ë¡œë“œ
    from endpoints.narrow_model import narrow_model
    print("âœ… Narrow model ready for inference")
```

---

#### Task 3: ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

**ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸**:
```python
# scripts/benchmark_inference.py

import asyncio
import aiohttp
import time
import numpy as np
from concurrent.futures import ThreadPoolExecutor
import json

class InferenceBenchmark:
    def __init__(self, server_url="http://localhost:8080"):
        self.server_url = server_url
        self.endpoint = f"{server_url}/v1/narrow/predict"

    async def single_request(self, image_path):
        """ë‹¨ì¼ ìš”ì²­ í…ŒìŠ¤íŠ¸"""
        async with aiohttp.ClientSession() as session:
            with open(image_path, 'rb') as f:
                data = aiohttp.FormData()
                data.add_field('file', f, filename='test.jpg')

                start = time.perf_counter()
                async with session.post(self.endpoint, data=data) as resp:
                    result = await resp.json()
                end = time.perf_counter()

                return {
                    'latency_ms': (end - start) * 1000,
                    'success': result.get('success', False)
                }

    async def concurrent_requests(self, image_path, num_requests=100):
        """ë™ì‹œ ìš”ì²­ í…ŒìŠ¤íŠ¸"""
        tasks = []
        for _ in range(num_requests):
            tasks.append(self.single_request(image_path))

        start = time.perf_counter()
        results = await asyncio.gather(*tasks)
        end = time.perf_counter()

        # í†µê³„ ê³„ì‚°
        latencies = [r['latency_ms'] for r in results if r['success']]
        success_count = sum(1 for r in results if r['success'])

        return {
            'total_time_s': end - start,
            'requests_per_second': num_requests / (end - start),
            'success_rate': success_count / num_requests,
            'latency_stats': {
                'mean': np.mean(latencies),
                'std': np.std(latencies),
                'p50': np.percentile(latencies, 50),
                'p95': np.percentile(latencies, 95),
                'p99': np.percentile(latencies, 99)
            }
        }

    def run_benchmark(self):
        """ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        print("="*50)
        print("Inference Server Benchmark")
        print("="*50)

        test_image = "test_images/sample_pill.jpg"

        # 1. Warmup
        print("\nğŸ“Š Warming up...")
        asyncio.run(self.concurrent_requests(test_image, 10))

        # 2. ë‹¨ì¼ ìš”ì²­ í…ŒìŠ¤íŠ¸
        print("\nğŸ“Š Single request latency...")
        latencies = []
        for _ in range(50):
            result = asyncio.run(self.single_request(test_image))
            latencies.append(result['latency_ms'])

        print(f"  Mean: {np.mean(latencies):.2f} ms")
        print(f"  P50: {np.percentile(latencies, 50):.2f} ms")
        print(f"  P95: {np.percentile(latencies, 95):.2f} ms")

        # 3. ë™ì‹œì„± í…ŒìŠ¤íŠ¸
        print("\nğŸ“Š Concurrent requests test...")
        for num_concurrent in [10, 50, 100]:
            print(f"\n  {num_concurrent} concurrent requests:")
            result = asyncio.run(
                self.concurrent_requests(test_image, num_concurrent)
            )
            print(f"    RPS: {result['requests_per_second']:.2f}")
            print(f"    Success rate: {result['success_rate']*100:.1f}%")
            print(f"    Mean latency: {result['latency_stats']['mean']:.2f} ms")
            print(f"    P95 latency: {result['latency_stats']['p95']:.2f} ms")

        # 4. ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸
        print("\nğŸ“Š Stress test (1000 requests)...")
        stress_result = asyncio.run(
            self.concurrent_requests(test_image, 1000)
        )
        print(f"  Total time: {stress_result['total_time_s']:.2f} s")
        print(f"  RPS: {stress_result['requests_per_second']:.2f}")
        print(f"  Success rate: {stress_result['success_rate']*100:.1f}%")

        return True

# ì‹¤í–‰
if __name__ == '__main__':
    benchmark = InferenceBenchmark()
    benchmark.run_benchmark()
```

---

#### Task 4: Docker ì´ë¯¸ì§€ ë¹Œë“œ ë° ë°°í¬

**Dockerfile**:
```dockerfile
# Dockerfile.narrow

FROM python:3.9-slim

# ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€
RUN apt-get update && apt-get install -y \
    libglib2.0-0 \
    libsm6 \
    libxext6 \
    libxrender-dev \
    libgomp1 \
    wget \
    && rm -rf /var/lib/apt/lists/*

# ì‘ì—… ë””ë ‰í† ë¦¬
WORKDIR /app

# ì˜ì¡´ì„± ì„¤ì¹˜
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ONNX Runtime GPU (ì„ íƒì )
# RUN pip install onnxruntime-gpu

# ì†ŒìŠ¤ ì½”ë“œ ë³µì‚¬
COPY src/ ./src/
COPY models/ ./models/

# í™˜ê²½ë³€ìˆ˜
ENV PYTHONUNBUFFERED=1
ENV MODEL_PATH=/app/models/pillsnap_narrow_model_quantized.onnx

# í¬íŠ¸
EXPOSE 8080

# ì‹¤í–‰
CMD ["uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "8080"]
```

**Docker Compose**:
```yaml
# docker-compose.narrow.yml

version: '3.8'

services:
  narrow-inference:
    build:
      context: .
      dockerfile: Dockerfile.narrow
    image: pillsnap-narrow:1.0.0
    container_name: pillsnap-narrow-inference
    ports:
      - "8081:8080"  # ë‹¤ë¥¸ í¬íŠ¸ ì‚¬ìš©
    environment:
      - MODEL_PATH=/app/models/pillsnap_narrow_model_quantized.onnx
      - DEVICE=cuda  # or cpu
      - LOG_LEVEL=info
    volumes:
      - ./models:/app/models:ro
      - ./logs:/app/logs
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  narrow-bff:
    image: pillsnap-bff:latest
    container_name: pillsnap-narrow-bff
    ports:
      - "8001:8000"  # ë‹¤ë¥¸ í¬íŠ¸
    environment:
      - INFERENCE_URL=http://narrow-inference:8080
      - NARROW_MODEL=true
    depends_on:
      - narrow-inference
    restart: unless-stopped
```

**ë°°í¬ ìŠ¤í¬ë¦½íŠ¸**:
```bash
#!/bin/bash
# deploy_narrow_model.sh

echo "ğŸš€ Deploying PillSnap Narrow Model..."

# 1. ëª¨ë¸ íŒŒì¼ í™•ì¸
if [ ! -f "models/pillsnap_narrow_model_quantized.onnx" ]; then
    echo "âŒ Model file not found!"
    exit 1
fi

# 2. Docker ì´ë¯¸ì§€ ë¹Œë“œ
echo "ğŸ“¦ Building Docker image..."
docker build -f Dockerfile.narrow -t pillsnap-narrow:1.0.0 .

# 3. ê¸°ì¡´ ì»¨í…Œì´ë„ˆ ì •ì§€
echo "ğŸ›‘ Stopping existing containers..."
docker-compose -f docker-compose.narrow.yml down

# 4. ìƒˆ ì»¨í…Œì´ë„ˆ ì‹œì‘
echo "ğŸš€ Starting new containers..."
docker-compose -f docker-compose.narrow.yml up -d

# 5. í—¬ìŠ¤ì²´í¬
echo "ğŸ¥ Health check..."
sleep 5
curl -f http://localhost:8081/health || exit 1

# 6. ëª¨ë¸ ì •ë³´ í™•ì¸
echo "ğŸ“Š Model info:"
curl http://localhost:8081/v1/narrow/info | jq .

echo "âœ… Deployment complete!"
```

---

### Part B: í…ŒìŠ¤íŠ¸

---

#### Task 5-6: ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ë° í†µí•© í…ŒìŠ¤íŠ¸

**í…ŒìŠ¤íŠ¸ êµ¬ì¡°**:
```python
# tests/test_narrow_model.py

import pytest
import numpy as np
from pathlib import Path
import json

class TestNarrowModel:
    @pytest.fixture
    def model(self):
        """ëª¨ë¸ fixture"""
        from endpoints.narrow_model import NarrowModelInference
        return NarrowModelInference()

    @pytest.fixture
    def test_images(self):
        """í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ fixture"""
        return {
            'tylenol': 'test_images/tylenol_500mg.jpg',
            'brufen': 'test_images/brufen_400mg.jpg',
            'aspirin': 'test_images/aspirin_100mg.jpg'
        }

    def test_model_loading(self, model):
        """ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸"""
        assert model.session is not None
        assert len(model.class_mapping['class_to_idx']) == 100

    def test_preprocessing(self, model):
        """ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        # ëœë¤ ì´ë¯¸ì§€
        image = np.random.randint(0, 255, (1024, 768, 3), dtype=np.uint8)
        tensor = model.preprocessor.process(image)

        assert tensor.shape == (3, 512, 512)
        assert tensor.dtype == torch.float32

    def test_single_prediction(self, model, test_images):
        """ë‹¨ì¼ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸"""
        with open(test_images['tylenol'], 'rb') as f:
            image_bytes = f.read()

        result = model.predict(image_bytes)

        assert result['success'] == True
        assert len(result['results']) == 5
        assert result['top1'] is not None
        assert 0 <= result['top1']['confidence'] <= 1

    def test_batch_prediction(self, model, test_images):
        """ë°°ì¹˜ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸"""
        images = []
        for path in test_images.values():
            with open(path, 'rb') as f:
                images.append(f.read())

        results = []
        for img in images:
            results.append(model.predict(img))

        assert all(r['success'] for r in results)

    def test_confidence_sum(self, model, test_images):
        """ì‹ ë¢°ë„ í•© í…ŒìŠ¤íŠ¸"""
        with open(test_images['tylenol'], 'rb') as f:
            result = model.predict(f.read())

        confidences = [r['confidence'] for r in result['results']]
        # Top-5 ì‹ ë¢°ë„ í•©ì´ ëŒ€ëµ 1ì— ê°€ê¹Œìš´ì§€
        assert 0.9 <= sum(confidences) <= 1.1

    @pytest.mark.parametrize("image_size", [
        (256, 256),
        (512, 512),
        (1024, 1024),
        (2048, 2048)
    ])
    def test_various_sizes(self, model, image_size):
        """ë‹¤ì–‘í•œ í¬ê¸° í…ŒìŠ¤íŠ¸"""
        image = np.random.randint(0, 255, (*image_size, 3), dtype=np.uint8)
        image_bytes = cv2.imencode('.jpg', image)[1].tobytes()

        result = model.predict(image_bytes)
        assert result['success'] == True

    def test_edge_cases(self, model):
        """ì—£ì§€ ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸"""

        # 1. ë§¤ìš° ì‘ì€ ì´ë¯¸ì§€
        small_img = np.ones((10, 10, 3), dtype=np.uint8) * 255
        small_bytes = cv2.imencode('.jpg', small_img)[1].tobytes()
        result = model.predict(small_bytes)
        assert result['success'] == True

        # 2. ë‹¨ìƒ‰ ì´ë¯¸ì§€
        blank_img = np.zeros((512, 512, 3), dtype=np.uint8)
        blank_bytes = cv2.imencode('.jpg', blank_img)[1].tobytes()
        result = model.predict(blank_bytes)
        assert result['success'] == True
```

**í†µí•© í…ŒìŠ¤íŠ¸**:
```python
# tests/test_integration.py

@pytest.mark.integration
class TestIntegration:
    def test_end_to_end_pipeline(self):
        """End-to-end íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""

        # 1. Flutter ì‹œë®¬ë ˆì´ì…˜ (ì´ë¯¸ì§€ ì „ì²˜ë¦¬)
        raw_image = load_test_image()
        preprocessed = simulate_flutter_preprocessing(raw_image)

        # 2. BFF ì„œë²„ë¡œ ì „ì†¡
        response = requests.post(
            'http://localhost:8001/v1/narrow/analyze',
            files={'file': preprocessed}
        )
        assert response.status_code == 200

        # 3. ê²°ê³¼ ê²€ì¦
        result = response.json()
        assert 'top1' in result
        assert 'drug_info' in result

    def test_concurrent_users(self):
        """ë™ì‹œ ì‚¬ìš©ì í…ŒìŠ¤íŠ¸"""
        import concurrent.futures

        def single_user_flow():
            # ì‚¬ìš©ì ì‹œë‚˜ë¦¬ì˜¤ ì‹œë®¬ë ˆì´ì…˜
            for _ in range(10):
                response = requests.post(
                    'http://localhost:8001/v1/narrow/analyze',
                    files={'file': get_random_test_image()}
                )
                assert response.status_code == 200

        # 20ëª… ë™ì‹œ ì‚¬ìš©ì
        with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:
            futures = [executor.submit(single_user_flow) for _ in range(20)]
            results = [f.result() for f in futures]

        assert all(r is None for r in results)  # No exceptions
```

---

#### Task 7: ì‹¤ì‚¬ì§„ ì •í™•ë„ í…ŒìŠ¤íŠ¸

**ì‹¤ì‚¬ì§„ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ êµ¬ì„±**:
```python
# scripts/test_real_photos.py

class RealPhotoTester:
    def __init__(self, model_endpoint, test_dataset_path):
        self.endpoint = model_endpoint
        self.test_path = Path(test_dataset_path)

        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
        self.test_data = self.load_test_data()

    def load_test_data(self):
        """ì‹¤ì‚¬ì§„ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ"""
        # Supabaseì—ì„œ ë‹¤ìš´ë¡œë“œí•œ ì‹¤ì‚¬ì§„
        test_data = []

        for kcode_dir in self.test_path.glob('*'):
            if not kcode_dir.is_dir():
                continue

            kcode = kcode_dir.name
            for img_path in kcode_dir.glob('*.jpg'):
                test_data.append({
                    'path': img_path,
                    'true_kcode': kcode,
                    'drug_name': self.get_drug_name(kcode)
                })

        print(f"âœ… Loaded {len(test_data)} test images")
        return test_data

    def test_accuracy(self):
        """ì •í™•ë„ í…ŒìŠ¤íŠ¸"""
        correct_top1 = 0
        correct_top5 = 0
        total = len(self.test_data)

        results = []

        for item in tqdm(self.test_data, desc="Testing"):
            # ì˜ˆì¸¡
            with open(item['path'], 'rb') as f:
                response = requests.post(
                    self.endpoint,
                    files={'file': f}
                )

            if response.status_code != 200:
                continue

            prediction = response.json()

            # Top-1 ì •í™•ë„
            if prediction['top1']['kcode'] == item['true_kcode']:
                correct_top1 += 1

            # Top-5 ì •í™•ë„
            top5_kcodes = [r['kcode'] for r in prediction['results'][:5]]
            if item['true_kcode'] in top5_kcodes:
                correct_top5 += 1

            # ê²°ê³¼ ì €ì¥
            results.append({
                'true': item['true_kcode'],
                'predicted': prediction['top1']['kcode'],
                'confidence': prediction['top1']['confidence'],
                'correct': prediction['top1']['kcode'] == item['true_kcode']
            })

        # ê²°ê³¼ ì¶œë ¥
        top1_acc = correct_top1 / total * 100
        top5_acc = correct_top5 / total * 100

        print(f"\nğŸ“Š Real Photo Test Results:")
        print(f"  Top-1 Accuracy: {top1_acc:.2f}% ({correct_top1}/{total})")
        print(f"  Top-5 Accuracy: {top5_acc:.2f}% ({correct_top5}/{total})")

        # í´ë˜ìŠ¤ë³„ ë¶„ì„
        self.analyze_per_class(results)

        return top1_acc, top5_acc

    def analyze_per_class(self, results):
        """í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„"""
        from collections import defaultdict

        class_stats = defaultdict(lambda: {'correct': 0, 'total': 0})

        for r in results:
            kcode = r['true']
            class_stats[kcode]['total'] += 1
            if r['correct']:
                class_stats[kcode]['correct'] += 1

        # ì •í™•ë„ ê³„ì‚°
        class_accuracies = []
        for kcode, stats in class_stats.items():
            acc = stats['correct'] / stats['total'] * 100
            class_accuracies.append({
                'kcode': kcode,
                'accuracy': acc,
                'samples': stats['total']
            })

        # ì •ë ¬
        class_accuracies.sort(key=lambda x: x['accuracy'])

        print("\nğŸ“Š Per-class Analysis:")
        print("\nğŸ”´ Worst 5 classes:")
        for item in class_accuracies[:5]:
            print(f"  {item['kcode']}: {item['accuracy']:.1f}% ({item['samples']} samples)")

        print("\nğŸŸ¢ Best 5 classes:")
        for item in class_accuracies[-5:]:
            print(f"  {item['kcode']}: {item['accuracy']:.1f}% ({item['samples']} samples)")
```

---

#### Task 8: ì•½êµ­ í˜„ì¥ í…ŒìŠ¤íŠ¸

**í˜„ì¥ í…ŒìŠ¤íŠ¸ í”„ë¡œí† ì½œ**:

**í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì •**:
```yaml
í…ŒìŠ¤íŠ¸ ì•½êµ­: 3ê³³
í…ŒìŠ¤íŠ¸ ê¸°ê°„: 3ì¼
í…ŒìŠ¤íŠ¸ ì¸ì›: ì•½ì‚¬ 3ëª…, ë³´ì¡° 3ëª…
ë””ë°”ì´ìŠ¤: Galaxy S21 (Flutter ì•± ì„¤ì¹˜)

í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤:
  1. ì¼ìƒ ì—…ë¬´ ì¤‘ í…ŒìŠ¤íŠ¸
  2. ë‹¤ì–‘í•œ ì¡°ëª… í™˜ê²½
  3. ì‹¤ì œ ì²˜ë°©ì „ ëŒ€ì¡°
  4. ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘
```

**í…ŒìŠ¤íŠ¸ ì²´í¬ë¦¬ìŠ¤íŠ¸**:
```markdown
## ì•½êµ­ í˜„ì¥ í…ŒìŠ¤íŠ¸ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
- [ ] ì•± ì‹¤í–‰ ë° ì¹´ë©”ë¼ ê¶Œí•œ
- [ ] ì´¬ì˜ ê°€ì´ë“œ UI í‘œì‹œ
- [ ] í’ˆì§ˆ ê²€ì¦ ì‘ë™
- [ ] ê²°ê³¼ í‘œì‹œ ì†ë„ (<3ì´ˆ)
- [ ] ì•½í’ˆ ì •ë³´ ì •í™•ë„

### ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
- [ ] ì—°ì† 10íšŒ ì´¬ì˜ ì•ˆì •ì„±
- [ ] ë„¤íŠ¸ì›Œí¬ ëŠê¹€ ëŒ€ì‘
- [ ] ë°°í„°ë¦¬ ì†Œëª¨ëŸ‰
- [ ] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰

### ì‚¬ìš©ì„± í…ŒìŠ¤íŠ¸
- [ ] UI ì§ê´€ì„± (5ì  ì²™ë„)
- [ ] ì´¬ì˜ ë‚œì´ë„ (5ì  ì²™ë„)
- [ ] ê²°ê³¼ ì‹ ë¢°ë„ (5ì  ì²™ë„)
- [ ] ì „ë°˜ì  ë§Œì¡±ë„ (5ì  ì²™ë„)

### ì •í™•ë„ ê²€ì¦
- [ ] Top-1 ì •í™•ë„: ____%
- [ ] Top-5 ì •í™•ë„: ____%
- [ ] ì˜¤ì¸ì‹ ì‚¬ë¡€ ê¸°ë¡
- [ ] ë¯¸ì¸ì‹ ì‚¬ë¡€ ê¸°ë¡
```

**í”¼ë“œë°± ìˆ˜ì§‘ ì–‘ì‹**:
```python
# scripts/collect_feedback.py

feedback_template = {
    'pharmacy_id': '',
    'tester_role': '',  # ì•½ì‚¬/ë³´ì¡°
    'test_date': '',
    'total_tests': 0,
    'successful_tests': 0,
    'metrics': {
        'avg_response_time': 0,
        'accuracy_perceived': 0,  # 1-5
        'ease_of_use': 0,  # 1-5
        'reliability': 0,  # 1-5
        'usefulness': 0  # 1-5
    },
    'issues': [],
    'suggestions': [],
    'would_recommend': False,
    'additional_comments': ''
}
```

---

## ğŸ“Š ìµœì¢… ì„±ê³¼ ê²€ì¦

### Success Criteria

| í•­ëª© | ëª©í‘œ | ë‹¬ì„± | ìƒíƒœ |
|------|------|------|------|
| **ëª¨ë¸ í¬ê¸°** | <100MB | ___MB | â³ |
| **ì¶”ë¡  ì†ë„** | <50ms | ___ms | â³ |
| **Top-1 ì •í™•ë„ (ì‹¤ì‚¬ì§„)** | >80% | ___% | â³ |
| **Top-5 ì •í™•ë„ (ì‹¤ì‚¬ì§„)** | >95% | ___% | â³ |
| **í˜„ì¥ ë§Œì¡±ë„** | >4.0/5.0 | ___/5.0 | â³ |
| **ì¼ì¼ ì²˜ë¦¬ëŸ‰** | >1000 | ___ | â³ |

### ìµœì¢… ê²€ì¦ ë¦¬í¬íŠ¸

```python
# scripts/generate_final_report.py

def generate_final_report():
    report = {
        'project': 'PillSnap Narrow Model',
        'version': '1.0.0',
        'date': datetime.now().isoformat(),
        'phases_completed': [
            'Data Preparation',
            'Collection System',
            'Photo Collection',
            'Model Training',
            'Deployment'
        ],
        'deliverables': {
            'model': 'pillsnap_narrow_model_quantized.onnx',
            'accuracy': {
                'top1': 0,  # Fill from tests
                'top5': 0
            },
            'coverage': '100 drugs',
            'dataset_size': 1350,
            'api_endpoint': '/v1/narrow/predict'
        },
        'performance': {
            'inference_time_p50': 0,
            'inference_time_p95': 0,
            'model_size_mb': 0,
            'memory_usage_mb': 0,
            'throughput_rps': 0
        },
        'field_test': {
            'pharmacies_tested': 3,
            'total_tests': 0,
            'user_satisfaction': 0
        }
    }

    # ë¦¬í¬íŠ¸ ìƒì„±
    with open('artifacts/final_report.json', 'w') as f:
        json.dump(report, f, indent=2)

    # PDF ìƒì„± (ì˜µì…˜)
    generate_pdf_report(report)

    return report
```

## ğŸ“ ìµœì¢… ì‚°ì¶œë¬¼

### ê¸°ìˆ  ì‚°ì¶œë¬¼
1. **ONNX ëª¨ë¸**: `pillsnap_narrow_model_quantized.onnx` (35MB)
2. **Docker ì´ë¯¸ì§€**: `pillsnap-narrow:1.0.0`
3. **API ì—”ë“œí¬ì¸íŠ¸**: `/v1/narrow/predict`
4. **Flutter APK**: `pill_snap_v1.0.0.apk`

### ë¬¸ì„œ ì‚°ì¶œë¬¼
1. **API ë¬¸ì„œ**: OpenAPI 3.0 ëª…ì„¸
2. **ì‚¬ìš©ì ê°€ì´ë“œ**: ì•½êµ­ìš© ë§¤ë‰´ì–¼
3. **ê¸°ìˆ  ë¬¸ì„œ**: ì•„í‚¤í…ì²˜ ë° êµ¬í˜„ ìƒì„¸
4. **í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸**: ì„±ëŠ¥ ë° ì •í™•ë„ ë³´ê³ ì„œ

### ë°ì´í„° ì‚°ì¶œë¬¼
1. **í•™ìŠµ ë°ì´í„°ì…‹**: 100ê°œ ì•½í’ˆ, 1,350ì¥
2. **í´ë˜ìŠ¤ ë§¤í•‘**: `class_mapping.json`
3. **ë©”íƒ€ë°ì´í„°**: ì•½í’ˆ ì •ë³´ DB

## ğŸ¯ í”„ë¡œì íŠ¸ ì™„ë£Œ

**í•µì‹¬ ì„±ê³¼**:
- âœ… 4,523ê°œ â†’ 100ê°œ ì•½í’ˆìœ¼ë¡œ ì§‘ì¤‘
- âœ… ì‹¤ì‚¬ì§„ ê¸°ë°˜ í•™ìŠµìœ¼ë¡œ ë„ë©”ì¸ ê°­ í•´ê²°
- âœ… 95% ëª©í‘œ ì •í™•ë„ ë‹¬ì„± (100ê°œ ì•½í’ˆ)
- âœ… 50ms ì´í•˜ ì¶”ë¡  ì†ë„
- âœ… ì•½êµ­ í˜„ì¥ ê²€ì¦ ì™„ë£Œ

**ë‹¤ìŒ ë‹¨ê³„**:
1. ì‚¬ìš©ì í”¼ë“œë°± ê¸°ë°˜ ê°œì„ 
2. ì¶”ê°€ ì•½í’ˆ í™•ì¥ (100 â†’ 200)
3. ë©€í‹°ëª¨ë‹¬ í†µí•© (í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€)
4. í´ë¼ìš°ë“œ ë°°í¬ ë° ìŠ¤ì¼€ì¼ë§