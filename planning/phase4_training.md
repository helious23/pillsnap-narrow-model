# ğŸ¤– Phase 4: ëª¨ë¸ í•™ìŠµ ìƒì„¸ ê³„íš

## ğŸ¯ ëª©í‘œ
100ê°œ ì•½í’ˆì— íŠ¹í™”ëœ **ê³ ì„±ëŠ¥ AI ëª¨ë¸** ê°œë°œ (Top-1 85%, Top-5 95% ì •í™•ë„)

## ğŸ“… ì¼ì •: 2024-11-02 ~ 2024-11-08 (7ì¼)

## ğŸ—ï¸ í•™ìŠµ ì•„í‚¤í…ì²˜

```mermaid
graph TB
    subgraph "ë°ì´í„° ì¤€ë¹„"
        A[ìŠ¤íŠœë””ì˜¤ ì´ë¯¸ì§€] --> C[í†µí•© ë°ì´í„°ì…‹]
        B[ì‹¤ì‚¬ì§„] --> C
    end

    subgraph "ì „ì²˜ë¦¬"
        C --> D[UnifiedPreprocessor]
        D --> E[512x512 í‘œì¤€í™”]
    end

    subgraph "í•™ìŠµ íŒŒì´í”„ë¼ì¸"
        E --> F[EfficientNetV2-S]
        F --> G[Domain Augmentation]
        G --> H[Mixed Precision Training]
    end

    subgraph "í‰ê°€"
        H --> I[Validation]
        I --> J[Real Photo Test]
        J --> K[ONNX Export]
    end
```

## ğŸ“‹ ìƒì„¸ ì‘ì—…

### Part A: ì „ì²˜ë¦¬ í‘œì¤€í™”

---

#### Task 1: UnifiedPreprocessor í´ë˜ìŠ¤ êµ¬í˜„

**ëª©ì **: Flutter, BFF, ì¶”ë¡ ì„œë²„ ê°„ ì¼ê´€ëœ ì „ì²˜ë¦¬ ë³´ì¥

**êµ¬í˜„ ì½”ë“œ**:
```python
# src/preprocessing/unified_preprocessor.py

import cv2
import numpy as np
from typing import Tuple, Optional
import torch
from torchvision import transforms

class UnifiedPreprocessor:
    """
    ëª¨ë“  íŒŒì´í”„ë¼ì¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” í‘œì¤€ ì „ì²˜ë¦¬ê¸°
    Flutter â†’ BFF â†’ Inference ì¼ê´€ì„± ë³´ì¥
    """

    def __init__(
        self,
        target_size: int = 512,
        enable_clahe: bool = True,
        clahe_clip_limit: float = 2.0,
        clahe_grid_size: int = 8,
        normalization_mean: Tuple[float, float, float] = (0.485, 0.456, 0.406),
        normalization_std: Tuple[float, float, float] = (0.229, 0.224, 0.225)
    ):
        self.target_size = target_size
        self.enable_clahe = enable_clahe
        self.clahe_clip_limit = clahe_clip_limit
        self.clahe_grid_size = clahe_grid_size
        self.mean = normalization_mean
        self.std = normalization_std

        # CLAHE ê°ì²´ ìƒì„±
        if enable_clahe:
            self.clahe = cv2.createCLAHE(
                clipLimit=clahe_clip_limit,
                tileGridSize=(clahe_grid_size, clahe_grid_size)
            )

    def process(self, image: np.ndarray) -> torch.Tensor:
        """
        ì´ë¯¸ì§€ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

        Args:
            image: RGB ì´ë¯¸ì§€ (H, W, 3)

        Returns:
            torch.Tensor: ì „ì²˜ë¦¬ëœ í…ì„œ (3, 512, 512)
        """
        # 1. í¬ê¸° ì¡°ì • (ì¤‘ì•™ í¬ë¡­ í›„ ë¦¬ì‚¬ì´ì¦ˆ)
        image = self._center_crop_and_resize(image)

        # 2. CLAHE ì ìš© (ì„ íƒì )
        if self.enable_clahe:
            image = self._apply_clahe(image)

        # 3. ì •ê·œí™”
        image = self._normalize(image)

        # 4. í…ì„œ ë³€í™˜
        tensor = torch.from_numpy(image).permute(2, 0, 1).float()

        return tensor

    def _center_crop_and_resize(self, image: np.ndarray) -> np.ndarray:
        """ì¤‘ì•™ í¬ë¡­ í›„ ì •ì‚¬ê°í˜• ë¦¬ì‚¬ì´ì¦ˆ"""
        h, w = image.shape[:2]

        # ì •ì‚¬ê°í˜• í¬ë¡­
        min_dim = min(h, w)
        start_h = (h - min_dim) // 2
        start_w = (w - min_dim) // 2
        cropped = image[start_h:start_h+min_dim, start_w:start_w+min_dim]

        # ë¦¬ì‚¬ì´ì¦ˆ
        resized = cv2.resize(
            cropped,
            (self.target_size, self.target_size),
            interpolation=cv2.INTER_LINEAR
        )

        return resized

    def _apply_clahe(self, image: np.ndarray) -> np.ndarray:
        """CLAHE (Contrast Limited Adaptive Histogram Equalization) ì ìš©"""
        # RGB to LAB
        lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)
        l, a, b = cv2.split(lab)

        # L ì±„ë„ì— CLAHE ì ìš©
        l_clahe = self.clahe.apply(l)

        # LAB to RGB
        lab_clahe = cv2.merge([l_clahe, a, b])
        rgb_clahe = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2RGB)

        return rgb_clahe

    def _normalize(self, image: np.ndarray) -> np.ndarray:
        """ImageNet ì •ê·œí™”"""
        image = image.astype(np.float32) / 255.0
        image = (image - self.mean) / self.std
        return image

    def get_config(self) -> dict:
        """ì „ì²˜ë¦¬ ì„¤ì • ë°˜í™˜ (ë‹¤ë¥¸ ì‹œìŠ¤í…œê³¼ ë™ê¸°í™”ìš©)"""
        return {
            'target_size': self.target_size,
            'enable_clahe': self.enable_clahe,
            'clahe_clip_limit': self.clahe_clip_limit,
            'clahe_grid_size': self.clahe_grid_size,
            'normalization_mean': self.mean,
            'normalization_std': self.std,
            'version': '1.0.0'
        }
```

**í…ŒìŠ¤íŠ¸ ì½”ë“œ**:
```python
def test_preprocessor_consistency():
    """ì „ì²˜ë¦¬ê¸° ì¼ê´€ì„± í…ŒìŠ¤íŠ¸"""
    preprocessor = UnifiedPreprocessor()

    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë¡œë“œ
    test_images = [
        'studio_sample.jpg',
        'real_photo_sample.jpg',
        'flutter_capture.jpg'
    ]

    results = []
    for img_path in test_images:
        image = cv2.imread(img_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        # ì „ì²˜ë¦¬
        tensor = preprocessor.process(image)

        # í†µê³„ í™•ì¸
        results.append({
            'path': img_path,
            'shape': tensor.shape,
            'mean': tensor.mean().item(),
            'std': tensor.std().item(),
            'min': tensor.min().item(),
            'max': tensor.max().item()
        })

    # ì¼ê´€ì„± ê²€ì¦
    assert all(r['shape'] == (3, 512, 512) for r in results)
    print("âœ… Preprocessor consistency test passed")
```

---

#### Task 2: Flutter ì´ë¯¸ì§€ ì „ì²˜ë¦¬

**Flutter ì „ì²˜ë¦¬ êµ¬í˜„**:
```dart
// lib/services/image_preprocessor.dart

import 'dart:typed_data';
import 'package:image/image.dart' as img;

class ImagePreprocessor {
  static const int targetSize = 512;
  static const double jpegQuality = 85;

  static Future<Uint8List> preprocess(Uint8List imageBytes) async {
    // 1. ì´ë¯¸ì§€ ë””ì½”ë“œ
    img.Image? image = img.decodeImage(imageBytes);
    if (image == null) throw Exception('Failed to decode image');

    // 2. ì¤‘ì•™ í¬ë¡­
    image = centerCrop(image);

    // 3. ë¦¬ì‚¬ì´ì¦ˆ
    image = img.copyResize(
      image,
      width: targetSize,
      height: targetSize,
      interpolation: img.Interpolation.linear,
    );

    // 4. JPEG ì••ì¶• (íŒŒì¼ í¬ê¸° ìµœì í™”)
    final processed = img.encodeJpg(
      image,
      quality: jpegQuality.round(),
    );

    return Uint8List.fromList(processed);
  }

  static img.Image centerCrop(img.Image image) {
    final width = image.width;
    final height = image.height;
    final minDim = width < height ? width : height;

    final startX = (width - minDim) ~/ 2;
    final startY = (height - minDim) ~/ 2;

    return img.copyCrop(
      image,
      x: startX,
      y: startY,
      width: minDim,
      height: minDim,
    );
  }

  static Map<String, dynamic> getConfig() {
    return {
      'target_size': targetSize,
      'jpeg_quality': jpegQuality,
      'interpolation': 'linear',
      'crop_method': 'center',
      'version': '1.0.0',
    };
  }
}
```

---

#### Task 3: BFF ê²€ì¦ ë¡œì§

**BFF ì´ë¯¸ì§€ ê²€ì¦**:
```python
# pillsnap_bff/src/utils/image_validator.py

class ImageValidator:
    """BFFì—ì„œ ì´ë¯¸ì§€ ê²€ì¦ (ë³€í™˜ ì—†ìŒ)"""

    @staticmethod
    def validate(image_bytes: bytes) -> Dict[str, any]:
        """
        ì´ë¯¸ì§€ ìœ íš¨ì„± ê²€ì‚¬ë§Œ ìˆ˜í–‰, ë³€í™˜ ì—†ìŒ
        """
        try:
            # ì´ë¯¸ì§€ ë¡œë“œ
            image = Image.open(io.BytesIO(image_bytes))

            # ê¸°ë³¸ ê²€ì¦
            validation_result = {
                'valid': True,
                'format': image.format,
                'mode': image.mode,
                'size': image.size,
                'file_size_kb': len(image_bytes) / 1024,
                'warnings': []
            }

            # í¬ê¸° ê²€ì¦
            width, height = image.size
            if width < 512 or height < 512:
                validation_result['warnings'].append(
                    f'Image resolution low: {width}x{height}'
                )

            # í¬ë§· ê²€ì¦
            if image.format not in ['JPEG', 'PNG']:
                validation_result['warnings'].append(
                    f'Unusual format: {image.format}'
                )

            # íŒŒì¼ í¬ê¸° ê²€ì¦
            if validation_result['file_size_kb'] > 10240:  # 10MB
                validation_result['valid'] = False
                validation_result['error'] = 'File size exceeds 10MB'

            return validation_result

        except Exception as e:
            return {
                'valid': False,
                'error': str(e)
            }

    @staticmethod
    def pass_through(image_bytes: bytes) -> bytes:
        """
        ì´ë¯¸ì§€ë¥¼ ë³€í™˜ ì—†ì´ ê·¸ëŒ€ë¡œ ì „ë‹¬
        BFFëŠ” ì¤‘ê°œì ì—­í• ë§Œ ìˆ˜í–‰
        """
        return image_bytes
```

---

#### Task 4: ì¶”ë¡ ì„œë²„ í†µí•©

**ì¶”ë¡ ì„œë²„ ì „ì²˜ë¦¬ í†µí•©**:
```python
# pillsnap_inference/src/preprocessing.py

from unified_preprocessor import UnifiedPreprocessor

class InferencePreprocessor:
    def __init__(self):
        self.preprocessor = UnifiedPreprocessor(
            target_size=512,
            enable_clahe=True
        )

    def prepare_for_model(self, image_bytes: bytes) -> torch.Tensor:
        """
        ëª¨ë¸ ì…ë ¥ì„ ìœ„í•œ ì „ì²˜ë¦¬
        """
        # ë°”ì´íŠ¸ì—ì„œ ì´ë¯¸ì§€ ë¡œë“œ
        nparr = np.frombuffer(image_bytes, np.uint8)
        image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        # í†µí•© ì „ì²˜ë¦¬ ì ìš©
        tensor = self.preprocessor.process(image)

        # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        tensor = tensor.unsqueeze(0)

        return tensor

    def prepare_batch(self, image_list: List[bytes]) -> torch.Tensor:
        """
        ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì „ì²˜ë¦¬
        """
        tensors = []
        for image_bytes in image_list:
            tensor = self.prepare_for_model(image_bytes)
            tensors.append(tensor)

        # ë°°ì¹˜ë¡œ ê²°í•©
        batch = torch.cat(tensors, dim=0)
        return batch
```

---

### Part B: ë°ì´í„°ì…‹ êµ¬ì„±

---

#### Task 5: 100ê°œ í´ë˜ìŠ¤ ì¸ë±ìŠ¤ ë§¤í•‘

**í´ë˜ìŠ¤ ë§¤í•‘ ìƒì„±**:
```python
# scripts/data_prep/create_class_mapping.py

import json
import pandas as pd

def create_class_mapping():
    """100ê°œ ì•½í’ˆì˜ í´ë˜ìŠ¤ ì¸ë±ìŠ¤ ë§¤í•‘"""

    # ìµœì¢… ì„ ì • 100ê°œ ì•½í’ˆ ë¡œë“œ
    with open('artifacts/datasets/top_100_drugs.json', 'r') as f:
        top_100 = json.load(f)

    # í´ë˜ìŠ¤ ì¸ë±ìŠ¤ ë§¤í•‘ (0-99)
    class_to_idx = {}
    idx_to_class = {}
    metadata = {}

    for idx, drug in enumerate(top_100['drugs']):
        kcode = drug['kcode']
        class_to_idx[kcode] = idx
        idx_to_class[idx] = kcode

        metadata[kcode] = {
            'index': idx,
            'drug_name': drug['drug_name'],
            'manufacturer': drug['manufacturer'],
            'form': drug['form'],
            'training_weight': drug['training_config']['weight']
        }

    # ì €ì¥
    mapping = {
        'class_to_idx': class_to_idx,
        'idx_to_class': idx_to_class,
        'num_classes': 100,
        'metadata': metadata,
        'version': '1.0.0'
    }

    output_path = 'artifacts/datasets/class_mapping.json'
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(mapping, f, ensure_ascii=False, indent=2)

    print(f"âœ… Class mapping created: {len(class_to_idx)} classes")
    return mapping
```

---

#### Task 6: ê¸°ì¡´ ìŠ¤íŠœë””ì˜¤ ì´ë¯¸ì§€ ì‹¬ë³¼ë¦­ ë§í¬

**ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„± ìŠ¤í¬ë¦½íŠ¸**:
```python
# scripts/data_prep/link_studio_images.py

import os
from pathlib import Path
import json

def create_studio_links():
    """ê¸°ì¡´ ìŠ¤íŠœë””ì˜¤ ì´ë¯¸ì§€ ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„±"""

    # ê²½ë¡œ ì„¤ì •
    source_base = Path('/mnt/windows/pillsnap_data/train')
    target_base = Path('artifacts/datasets/narrow_dataset/studio')

    # í´ë˜ìŠ¤ ë§¤í•‘ ë¡œë“œ
    with open('artifacts/datasets/class_mapping.json', 'r') as f:
        mapping = json.load(f)

    # 100ê°œ K-CODEë§Œ í•„í„°ë§
    selected_kcodes = mapping['class_to_idx'].keys()

    # ë””ë ‰í† ë¦¬ ìƒì„±
    target_base.mkdir(parents=True, exist_ok=True)

    # ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„±
    linked_count = 0
    for kcode in selected_kcodes:
        source_dir = source_base / kcode
        target_dir = target_base / kcode

        if source_dir.exists():
            # ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„±
            if not target_dir.exists():
                os.symlink(source_dir, target_dir)
                linked_count += 1

            # ì´ë¯¸ì§€ ê°œìˆ˜ í™•ì¸
            image_count = len(list(source_dir.glob('*.jpg')))
            print(f"  {kcode}: {image_count} images")

    print(f"âœ… Linked {linked_count} K-CODE directories")
```

---

#### Task 7: ì‹¤ì‚¬ì§„ í†µí•© - 3:7 ë¹„ìœ¨

**ë°ì´í„° í†µí•© ìŠ¤í¬ë¦½íŠ¸**:
```python
# scripts/data_prep/integrate_real_photos.py

import shutil
from pathlib import Path
import random
import json

def integrate_datasets():
    """
    ìŠ¤íŠœë””ì˜¤:ì‹¤ì‚¬ì§„ = 3:7 ë¹„ìœ¨ë¡œ í†µí•©
    """

    # ê²½ë¡œ ì„¤ì •
    studio_base = Path('artifacts/datasets/narrow_dataset/studio')
    real_base = Path('artifacts/datasets/narrow_dataset/real_photos')
    combined_base = Path('artifacts/datasets/narrow_dataset/combined')

    # í´ë˜ìŠ¤ë³„ ë¹„ìœ¨ ì„¤ì •
    STUDIO_RATIO = 0.3
    REAL_RATIO = 0.7
    MIN_IMAGES_PER_CLASS = 50

    stats = {
        'total_studio': 0,
        'total_real': 0,
        'total_combined': 0,
        'per_class': {}
    }

    # í´ë˜ìŠ¤ ë§¤í•‘ ë¡œë“œ
    with open('artifacts/datasets/class_mapping.json', 'r') as f:
        mapping = json.load(f)

    for kcode in mapping['class_to_idx'].keys():
        combined_dir = combined_base / kcode
        combined_dir.mkdir(parents=True, exist_ok=True)

        # ìŠ¤íŠœë””ì˜¤ ì´ë¯¸ì§€
        studio_images = list((studio_base / kcode).glob('*.jpg'))
        real_images = list((real_base / kcode).glob('*.jpg'))

        # ë¹„ìœ¨ ê³„ì‚°
        total_needed = max(len(real_images) / REAL_RATIO, MIN_IMAGES_PER_CLASS)
        n_studio = int(total_needed * STUDIO_RATIO)
        n_real = int(total_needed * REAL_RATIO)

        # ìƒ˜í”Œë§
        selected_studio = random.sample(
            studio_images,
            min(n_studio, len(studio_images))
        )
        selected_real = random.sample(
            real_images,
            min(n_real, len(real_images))
        )

        # ë³µì‚¬ ë˜ëŠ” ì‹¬ë³¼ë¦­ ë§í¬
        for img in selected_studio:
            target = combined_dir / f"studio_{img.name}"
            if not target.exists():
                os.symlink(img, target)

        for img in selected_real:
            target = combined_dir / f"real_{img.name}"
            if not target.exists():
                shutil.copy2(img, target)

        # í†µê³„ ì—…ë°ì´íŠ¸
        stats['per_class'][kcode] = {
            'studio': len(selected_studio),
            'real': len(selected_real),
            'total': len(selected_studio) + len(selected_real)
        }
        stats['total_studio'] += len(selected_studio)
        stats['total_real'] += len(selected_real)
        stats['total_combined'] += len(selected_studio) + len(selected_real)

    # í†µê³„ ì €ì¥
    with open('artifacts/datasets/integration_stats.json', 'w') as f:
        json.dump(stats, f, indent=2)

    print(f"âœ… Dataset integrated:")
    print(f"  Studio: {stats['total_studio']} ({STUDIO_RATIO*100:.0f}%)")
    print(f"  Real: {stats['total_real']} ({REAL_RATIO*100:.0f}%)")
    print(f"  Total: {stats['total_combined']}")
```

---

#### Task 8: Train/Val/Test ë¶„í• 

**ë°ì´í„° ë¶„í• **:
```python
# scripts/data_prep/split_dataset.py

from sklearn.model_selection import train_test_split
import json
from pathlib import Path

def split_dataset():
    """
    80:10:10 ë¹„ìœ¨ë¡œ ë°ì´í„° ë¶„í• 
    í´ë˜ìŠ¤ ê· í˜• ìœ ì§€
    """

    base_path = Path('artifacts/datasets/narrow_dataset/combined')

    # ë¶„í•  ë¹„ìœ¨
    TRAIN_RATIO = 0.8
    VAL_RATIO = 0.1
    TEST_RATIO = 0.1

    # í´ë˜ìŠ¤ ë§¤í•‘ ë¡œë“œ
    with open('artifacts/datasets/class_mapping.json', 'r') as f:
        mapping = json.load(f)

    splits = {
        'train': [],
        'val': [],
        'test': []
    }

    # í´ë˜ìŠ¤ë³„ë¡œ ë¶„í• 
    for kcode, idx in mapping['class_to_idx'].items():
        class_dir = base_path / kcode
        images = list(class_dir.glob('*.jpg'))

        # ì„ê¸°
        random.shuffle(images)

        # 8:1:1 ë¶„í• 
        n_total = len(images)
        n_train = int(n_total * TRAIN_RATIO)
        n_val = int(n_total * VAL_RATIO)

        train_images = images[:n_train]
        val_images = images[n_train:n_train+n_val]
        test_images = images[n_train+n_val:]

        # ê²½ë¡œì™€ ë ˆì´ë¸” ì €ì¥
        for img in train_images:
            splits['train'].append({
                'path': str(img),
                'label': idx,
                'kcode': kcode
            })

        for img in val_images:
            splits['val'].append({
                'path': str(img),
                'label': idx,
                'kcode': kcode
            })

        for img in test_images:
            splits['test'].append({
                'path': str(img),
                'label': idx,
                'kcode': kcode
            })

    # í†µê³„
    print(f"âœ… Dataset split complete:")
    print(f"  Train: {len(splits['train'])} images")
    print(f"  Val: {len(splits['val'])} images")
    print(f"  Test: {len(splits['test'])} images")

    # ì €ì¥
    with open('artifacts/datasets/splits.json', 'w') as f:
        json.dump(splits, f, indent=2)

    return splits
```

---

#### Task 9: Manifest JSON íŒŒì¼ ìƒì„±

**Manifest ìƒì„±**:
```python
# scripts/data_prep/create_manifest.py

def create_manifest():
    """
    í•™ìŠµìš© Manifest íŒŒì¼ ìƒì„±
    """

    manifest = {
        'dataset_name': 'PillSnap Narrow Model Dataset',
        'version': '1.0.0',
        'created_date': datetime.now().isoformat(),
        'num_classes': 100,
        'total_images': 0,
        'splits': {},
        'class_distribution': {},
        'preprocessing': UnifiedPreprocessor().get_config(),
        'augmentation': {
            'train': {
                'random_rotation': 30,
                'color_jitter': {
                    'brightness': 0.2,
                    'contrast': 0.2,
                    'saturation': 0.2,
                    'hue': 0.1
                },
                'random_crop': {
                    'scale': [0.8, 1.0],
                    'ratio': [0.9, 1.1]
                },
                'gaussian_blur': {
                    'kernel_size': 5,
                    'sigma': [0.1, 2.0]
                }
            },
            'val': None,  # No augmentation for validation
            'test': None  # No augmentation for test
        }
    }

    # ë¶„í•  ë°ì´í„° ë¡œë“œ
    with open('artifacts/datasets/splits.json', 'r') as f:
        splits = json.load(f)

    # í†µê³„ ê³„ì‚°
    for split_name, split_data in splits.items():
        manifest['splits'][split_name] = {
            'num_images': len(split_data),
            'class_distribution': {}
        }

        # í´ë˜ìŠ¤ë³„ ë¶„í¬
        class_counts = {}
        for item in split_data:
            kcode = item['kcode']
            class_counts[kcode] = class_counts.get(kcode, 0) + 1

        manifest['splits'][split_name]['class_distribution'] = class_counts
        manifest['total_images'] += len(split_data)

    # ì „ì²´ í´ë˜ìŠ¤ ë¶„í¬
    all_classes = {}
    for split in manifest['splits'].values():
        for kcode, count in split['class_distribution'].items():
            all_classes[kcode] = all_classes.get(kcode, 0) + count

    manifest['class_distribution'] = all_classes

    # ì €ì¥
    with open('artifacts/datasets/manifest.json', 'w') as f:
        json.dump(manifest, f, indent=2)

    print(f"âœ… Manifest created:")
    print(f"  Total images: {manifest['total_images']}")
    print(f"  Classes: {manifest['num_classes']}")
    print(f"  Version: {manifest['version']}")

    return manifest
```

---

### Part C: ëª¨ë¸ í•™ìŠµ

---

#### Task 10-12: í”„ë¡œì íŠ¸ êµ¬ì¡° ë° í•™ìŠµ ì¤€ë¹„

**í•™ìŠµ í”„ë¡œì íŠ¸ êµ¬ì¡°**:
```bash
# ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
mkdir -p pillsnap_narrow/{
    src/{models,training,data,utils},
    configs,
    scripts,
    checkpoints,
    logs,
    artifacts
}
```

**í•™ìŠµ ì„¤ì • YAML**:
```yaml
# configs/train_config.yaml

model:
  name: tf_efficientnetv2_s
  pretrained: true
  num_classes: 100
  drop_rate: 0.3
  drop_path_rate: 0.2

data:
  manifest_path: artifacts/datasets/manifest.json
  input_size: 512
  batch_size: 32
  num_workers: 8
  pin_memory: true
  persistent_workers: true

training:
  epochs: 50
  warmup_epochs: 3
  base_lr: 1e-3
  min_lr: 1e-6
  weight_decay: 5e-4
  label_smoothing: 0.1
  gradient_clip: 1.0

optimizer:
  name: AdamW
  betas: [0.9, 0.999]
  eps: 1e-8

scheduler:
  name: cosine
  warmup_method: linear
  warmup_factor: 0.001

augmentation:
  mixup_alpha: 0.2
  cutmix_alpha: 1.0
  cutmix_minmax: [0.1, 1.0]
  mixup_prob: 0.5

validation:
  frequency: 1  # Validate every epoch
  save_best: true
  metrics: [top1, top5, loss]

checkpoint:
  save_frequency: 5
  keep_best: 3
  resume: null

logging:
  tensorboard: true
  wandb: false
  log_frequency: 100
```

**ë°ì´í„° ë¡œë” êµ¬í˜„**:
```python
# src/data/dataloader.py

class NarrowModelDataset(Dataset):
    def __init__(self, split='train', transform=None):
        # Manifest ë¡œë“œ
        with open('artifacts/datasets/manifest.json', 'r') as f:
            self.manifest = json.load(f)

        # Split ë°ì´í„° ë¡œë“œ
        with open('artifacts/datasets/splits.json', 'r') as f:
            splits = json.load(f)

        self.data = splits[split]
        self.transform = transform
        self.preprocessor = UnifiedPreprocessor()

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]

        # ì´ë¯¸ì§€ ë¡œë“œ
        image = cv2.imread(item['path'])
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        # ì „ì²˜ë¦¬
        tensor = self.preprocessor.process(image)

        # Augmentation (í•™ìŠµì‹œì—ë§Œ)
        if self.transform:
            tensor = self.transform(tensor)

        label = item['label']

        return tensor, label

def create_dataloaders(config):
    # Transform ì •ì˜
    train_transform = transforms.Compose([
        transforms.RandomRotation(30),
        transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),
        transforms.RandomErasing(p=0.2),
    ])

    # ë°ì´í„°ì…‹ ìƒì„±
    train_dataset = NarrowModelDataset('train', train_transform)
    val_dataset = NarrowModelDataset('val', None)
    test_dataset = NarrowModelDataset('test', None)

    # ë°ì´í„°ë¡œë” ìƒì„±
    train_loader = DataLoader(
        train_dataset,
        batch_size=config['data']['batch_size'],
        shuffle=True,
        num_workers=config['data']['num_workers'],
        pin_memory=config['data']['pin_memory'],
        persistent_workers=config['data']['persistent_workers']
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=config['data']['batch_size'],
        shuffle=False,
        num_workers=config['data']['num_workers'],
        pin_memory=config['data']['pin_memory']
    )

    test_loader = DataLoader(
        test_dataset,
        batch_size=config['data']['batch_size'],
        shuffle=False,
        num_workers=config['data']['num_workers'],
        pin_memory=config['data']['pin_memory']
    )

    return train_loader, val_loader, test_loader
```

---

#### Task 13-14: ëª¨ë¸ í•™ìŠµ ì‹¤í–‰

**í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸**:
```python
# src/training/train.py

import torch
import torch.nn as nn
import timm
from torch.cuda.amp import GradScaler, autocast
import wandb
from tqdm import tqdm

class NarrowModelTrainer:
    def __init__(self, config):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # ëª¨ë¸ ìƒì„±
        self.model = self._create_model()

        # ì˜µí‹°ë§ˆì´ì € & ìŠ¤ì¼€ì¤„ëŸ¬
        self.optimizer = self._create_optimizer()
        self.scheduler = self._create_scheduler()

        # Loss & Metrics
        self.criterion = nn.CrossEntropyLoss(
            label_smoothing=config['training']['label_smoothing']
        )
        self.scaler = GradScaler()  # Mixed precision

        # ë¡œê¹…
        self.setup_logging()

    def _create_model(self):
        model = timm.create_model(
            self.config['model']['name'],
            pretrained=self.config['model']['pretrained'],
            num_classes=self.config['model']['num_classes'],
            drop_rate=self.config['model']['drop_rate'],
            drop_path_rate=self.config['model']['drop_path_rate']
        )
        return model.to(self.device)

    def _create_optimizer(self):
        return torch.optim.AdamW(
            self.model.parameters(),
            lr=self.config['training']['base_lr'],
            weight_decay=self.config['training']['weight_decay'],
            betas=self.config['optimizer']['betas']
        )

    def _create_scheduler(self):
        return torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=self.config['training']['epochs'],
            eta_min=self.config['training']['min_lr']
        )

    def train_epoch(self, train_loader, epoch):
        self.model.train()
        running_loss = 0.0
        correct = 0
        total = 0

        pbar = tqdm(train_loader, desc=f'Epoch {epoch}')
        for batch_idx, (inputs, targets) in enumerate(pbar):
            inputs, targets = inputs.to(self.device), targets.to(self.device)

            # Mixed precision training
            with autocast():
                outputs = self.model(inputs)
                loss = self.criterion(outputs, targets)

            # Backward
            self.optimizer.zero_grad()
            self.scaler.scale(loss).backward()

            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(
                self.model.parameters(),
                self.config['training']['gradient_clip']
            )

            self.scaler.step(self.optimizer)
            self.scaler.update()

            # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

            # í”„ë¡œê·¸ë ˆìŠ¤ ë°” ì—…ë°ì´íŠ¸
            pbar.set_postfix({
                'loss': running_loss / (batch_idx + 1),
                'acc': 100. * correct / total
            })

            # ë¡œê¹…
            if batch_idx % self.config['logging']['log_frequency'] == 0:
                self.log_step(epoch, batch_idx, loss.item(), correct/total)

        return running_loss / len(train_loader), correct / total

    def validate(self, val_loader, epoch):
        self.model.eval()
        running_loss = 0.0
        correct = 0
        correct_top5 = 0
        total = 0

        with torch.no_grad():
            for inputs, targets in tqdm(val_loader, desc='Validation'):
                inputs, targets = inputs.to(self.device), targets.to(self.device)

                outputs = self.model(inputs)
                loss = self.criterion(outputs, targets)

                running_loss += loss.item()
                _, predicted = outputs.max(1)
                total += targets.size(0)
                correct += predicted.eq(targets).sum().item()

                # Top-5 accuracy
                _, pred_top5 = outputs.topk(5, 1, True, True)
                pred_top5 = pred_top5.t()
                correct_top5 += pred_top5.eq(targets.view(1, -1).expand_as(pred_top5)).sum().item()

        val_loss = running_loss / len(val_loader)
        top1_acc = correct / total
        top5_acc = correct_top5 / total

        print(f'Validation - Loss: {val_loss:.4f}, Top-1: {top1_acc*100:.2f}%, Top-5: {top5_acc*100:.2f}%')

        return val_loss, top1_acc, top5_acc

    def train(self, train_loader, val_loader, test_loader):
        best_acc = 0

        for epoch in range(self.config['training']['epochs']):
            # Training
            train_loss, train_acc = self.train_epoch(train_loader, epoch)

            # Validation
            val_loss, val_top1, val_top5 = self.validate(val_loader, epoch)

            # Scheduler step
            self.scheduler.step()

            # Save checkpoint
            if val_top1 > best_acc:
                best_acc = val_top1
                self.save_checkpoint(epoch, val_top1, is_best=True)

            # Logging
            self.log_epoch(epoch, train_loss, train_acc, val_loss, val_top1, val_top5)

        # Final test
        print("Running final test...")
        test_loss, test_top1, test_top5 = self.validate(test_loader, -1)
        print(f'Test Results - Top-1: {test_top1*100:.2f}%, Top-5: {test_top5*100:.2f}%')

    def save_checkpoint(self, epoch, accuracy, is_best=False):
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'accuracy': accuracy,
            'config': self.config
        }

        path = f'checkpoints/checkpoint_epoch_{epoch}.pth'
        torch.save(checkpoint, path)

        if is_best:
            torch.save(checkpoint, 'checkpoints/best_model.pth')
            print(f'âœ… Best model saved with accuracy: {accuracy*100:.2f}%')

# í•™ìŠµ ì‹¤í–‰
if __name__ == '__main__':
    # ì„¤ì • ë¡œë“œ
    with open('configs/train_config.yaml', 'r') as f:
        config = yaml.load(f, Loader=yaml.FullLoader)

    # ë°ì´í„°ë¡œë” ìƒì„±
    train_loader, val_loader, test_loader = create_dataloaders(config)

    # íŠ¸ë ˆì´ë„ˆ ìƒì„± ë° í•™ìŠµ
    trainer = NarrowModelTrainer(config)
    trainer.train(train_loader, val_loader, test_loader)
```

---

## ğŸ“Š ì„±ëŠ¥ í‰ê°€ ë° ìµœì í™”

### í‰ê°€ ë©”íŠ¸ë¦­

| ë©”íŠ¸ë¦­ | ëª©í‘œ | ìµœì†Œ | ì¸¡ì • ë°©ë²• |
|--------|------|------|-----------|
| **Top-1 Accuracy** | 85% | 80% | Test set |
| **Top-5 Accuracy** | 95% | 92% | Test set |
| **Real Photo Top-1** | 80% | 75% | Real photos only |
| **Inference Speed** | <50ms | <100ms | ONNX runtime |
| **Model Size** | <100MB | <150MB | ONNX file |

### ì„±ëŠ¥ ë¶„ì„ ë„êµ¬

```python
# scripts/analyze_performance.py

def analyze_model_performance(model_path, test_loader):
    """ëª¨ë¸ ì„±ëŠ¥ ìƒì„¸ ë¶„ì„"""

    results = {
        'overall': {},
        'per_class': {},
        'confusion_matrix': None,
        'failure_analysis': []
    }

    # í´ë˜ìŠ¤ë³„ ì •í™•ë„
    class_correct = defaultdict(int)
    class_total = defaultdict(int)

    # Confusion matrix
    all_preds = []
    all_targets = []

    # ì˜ˆì¸¡ ìˆ˜í–‰
    for inputs, targets in test_loader:
        outputs = model(inputs)
        _, preds = outputs.max(1)

        all_preds.extend(preds.cpu().numpy())
        all_targets.extend(targets.cpu().numpy())

        # í´ë˜ìŠ¤ë³„ í†µê³„
        for pred, target in zip(preds, targets):
            class_total[target.item()] += 1
            if pred == target:
                class_correct[target.item()] += 1

    # Confusion Matrix
    results['confusion_matrix'] = confusion_matrix(all_targets, all_preds)

    # í´ë˜ìŠ¤ë³„ ì •í™•ë„ ê³„ì‚°
    for class_idx in range(100):
        if class_total[class_idx] > 0:
            acc = class_correct[class_idx] / class_total[class_idx]
            results['per_class'][class_idx] = {
                'accuracy': acc,
                'total': class_total[class_idx],
                'correct': class_correct[class_idx]
            }

            # ì €ì„±ëŠ¥ í´ë˜ìŠ¤ ë¶„ì„
            if acc < 0.7:
                results['failure_analysis'].append({
                    'class': class_idx,
                    'accuracy': acc,
                    'samples': class_total[class_idx]
                })

    return results
```

## ğŸ“ ì‚°ì¶œë¬¼

1. **ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸**:
   - `best_model.pth` - ìµœê³  ì„±ëŠ¥ ëª¨ë¸
   - `checkpoint_epoch_*.pth` - ì—í­ë³„ ì²´í¬í¬ì¸íŠ¸

2. **í•™ìŠµ ë¡œê·¸**:
   - TensorBoard ë¡œê·¸
   - í•™ìŠµ ê³¡ì„  ê·¸ë˜í”„
   - ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¦¬í¬íŠ¸

3. **ë°ì´í„°ì…‹**:
   - í†µí•© ë°ì´í„°ì…‹ (ìŠ¤íŠœë””ì˜¤ + ì‹¤ì‚¬ì§„)
   - Train/Val/Test ë¶„í• 
   - Manifest íŒŒì¼

## ğŸ”— ë‹¤ìŒ ë‹¨ê³„
Phase 5: ëª¨ë¸ ë°°í¬ - ONNX ë³€í™˜, ì¶”ë¡  ì„œë²„ í†µí•©, ì‹¤ì „ í…ŒìŠ¤íŠ¸